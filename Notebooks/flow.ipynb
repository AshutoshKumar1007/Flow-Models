{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d70d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    the Convnet that predicts (log s,t) from the masked input.\n",
    "    TODO we have to be carefull about 1-d,2-d,3-d data also \n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        super(ConvNet,self).__init__()\n",
    "        self.NN = None\n",
    "        #! head must be initialized with zero wieghts,bias...why? \n",
    "    def forward(self,x):\n",
    "        \"\"\" \n",
    "        TODO - should work for 1-d,2-d,3d data as well       \n",
    "        Args:\n",
    "            x (torch.Tensor): shape: (B,C,H,W)\n",
    "        return: \n",
    "            log s,t (torch.Tensor): shape: (B,C,H,W)\n",
    "        \"\"\"\n",
    "        h = self.NN(x) #shape: (B,2C,H,W)\n",
    "        log_s,t = torch.chunk(h,2,dim=1) # shape: (B,C,H,W)\n",
    "        #TODO unconstraint log_s... won't that be problem\n",
    "        return log_s,t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba1a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    \"\"\"\n",
    "    Affine coupling layer with given binary mask.\n",
    "    \"\"\"\n",
    "    def __init__(self,mask: torch.Tensor):\n",
    "        super(self,AffineCoupling).__init__()\n",
    "        self.convnet = ConvNet()\n",
    "        self.register_buffer('mask',mask)\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        split x->(x1,x2) using the mask.\n",
    "        cumpute logs,t -> NN(x1)\n",
    "        apply affine transform on y2.\n",
    "        Args:\n",
    "            x (torch.Tensor): input\n",
    "            log_det (torch.Tensor, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        #split\n",
    "        x1 = self.mask*x\n",
    "        x2 = (1 - self.mask)*x\n",
    "        #transform\n",
    "        log_s,t = self.convnet(x1)\n",
    "        y2 = x2*torch.exp(log_s) + t\n",
    "        #squeeze\n",
    "        y = x1 + y2 \n",
    "        log_det = torch.sum((log_s*(1 - self.mask)),dim=(1,2,3)) #shape: (B,)\n",
    "        return y,log_det\n",
    "    def backward(self,y):\n",
    "        y1 = self.mask*y\n",
    "        y2 = (self.mask - 1)*y\n",
    "        log_s,t = self.convnet(y1)\n",
    "        x2 = y2*torch.exp(-log_s) - t\n",
    "        x = y1 + x2\n",
    "        log_det = torch.sum((-log_s*(1 - self.mask)),dim=(1,2,3))\n",
    "        return x,log_det    \n",
    "                        \n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_channels: int, eps : float = 1e-8,momentum: float = 0.1,affine : bool = False):\n",
    "        super(BatchNorm,self).__init__()\n",
    "        self.affine = affine\n",
    "        self.eps = eps \n",
    "        self.momemtum = momentum\n",
    "        self.batchnorm = nn.BatchNorm2d(num_channels,momentum=momentum,affine=self.affine,eps = self.eps) # dim = x[,:::].dim\n",
    "        #TODO ig, we have to take care of initialization of stats...\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        affine = False, doesn't apply the affine transform\n",
    "        #TODO careful about the training and inference phase\n",
    "        #? should be take mean over batch also alongside or maybe later (ig, it would be more stable if be normalize it here only...)\n",
    "        Args:\n",
    "            x (torch.Tensor): input\n",
    "        \"\"\"\n",
    "        B,C,H,W = x.shape\n",
    "        y = self.batchnorm(x)\n",
    "        # mu,var (per channel statistics)-> one scalar per channel\n",
    "        if not self.affine:\n",
    "            log_det = -0.5*H*W*torch.sum(torch.log(self.batchnorm.running_var + self.batchnorm.eps),dim = 1) #shape:(B,)  \n",
    "            return y,log_det  \n",
    "        else:\n",
    "            log_det = H*W*torch.sum(torch.sum(torch.log(self.batchnorm.weight) - 0.5*torch.log(self.batchnorm.running_var + self.batchnorm.eps)),dim = 1) #()\n",
    "            return y,log_det                          \n",
    "    def backward(self,y:torch.Tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): input\n",
    "        \"\"\"\n",
    "        B,C,H,W = y.shape\n",
    "        sigma = (self.batchnorm.running_var + self.batchnorm.eps)**(-0.5)\n",
    "        mu = -self.batchnorm.running_mean*(sigma**-1)\n",
    "        x = y*sigma + mu\n",
    "        if not self.affine:\n",
    "            log_det = 0.5*H*W*torch.sum(torch.log(self.batchnorm.running_var + self.batchnorm.eps),dim= 0) #shape:(B,) #! ig, dim should be 0(default), as w're summing over C only \n",
    "            return x,log_det\n",
    "        else:\n",
    "            log_det = H*W*torch.sum(torch.sum( - torch.log(self.batchnorm.weight) + 0.5*torch.log(self.batchnorm.running_var + self.batchnorm.eps)),dim = 0) #! because these stats are for per channel for whole batch\n",
    "            return x,log_det\n",
    "        \n",
    "        \n",
    "class ActNorm(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self,num_channels:int,eps : float = 1e-5):\n",
    "        super(ActNorm,self).__init__()\n",
    "        self.m = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.init = False\n",
    "        self.eps = eps\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        initialization on the first forward pass, mc,bc are set such that \n",
    "        the batch of activations has zero mean and unit variance.\n",
    "        thereafter, there are considered as learnable parameters.\n",
    "        Args:\n",
    "            x (torch.Tensor): _description_\n",
    "        \"\"\"\n",
    "        B,C,H,W = x.shape\n",
    "        if not self.init:\n",
    "            with torch.no_grad():\n",
    "                self.m = torch.log(torch.std(x,dim=(0,2,3)) + self.eps) #shape: (C,)\n",
    "                self.bias = torch.mean(x,dim=(0,2,3)) #shape: (C,)\n",
    "        y = (x + self.bias)*torch.exp(-self.m)\n",
    "        log_det = H*W*torch.sum(x,dim=0)\n",
    "        return y,log_det\n",
    "    def backward(self,y:torch.Tensor):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): input\n",
    "        \"\"\"\n",
    "        B,C,H,W = y.shape\n",
    "        x = y*torch.exp(self.m) - self.bias\n",
    "        log_det = -H*W*torch.sum(self.m,dim=0)\n",
    "        return x,log_det\n",
    "\n",
    "\n",
    "\n",
    "class Invertible1x1_conv(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Invertible1x1_conv,self).__init__()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "              \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981b48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
